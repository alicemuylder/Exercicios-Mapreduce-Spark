{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06e94bb",
   "metadata": {},
   "source": [
    "O exercício 5.2 faz uma análise de logs de servidor web usando PySpark:\n",
    "\n",
    "    Lê o arquivo de log e extrai informações importantes usando expressões regulares:\n",
    "\n",
    "    IP do visitante\n",
    "\n",
    "    Timestamp do acesso\n",
    "\n",
    "    Método HTTP (GET, POST…)\n",
    "\n",
    "    URL acessada\n",
    "\n",
    "    Status HTTP (200, 404, 500…)\n",
    "\n",
    "    Tamanho da resposta\n",
    "\n",
    "    Cria um DataFrame estruturado (parsed_logs) e converte o timestamp em formato datetime.\n",
    "\n",
    "\n",
    "\n",
    "Realiza quatro análises principais:\n",
    "\n",
    "    Top 10 URLs mais acessadas → quais páginas recebem mais visitas.\n",
    "\n",
    "    Distribuição de status HTTP → quantos acessos retornaram 200, 404, 500, etc.\n",
    "\n",
    "    Acessos por hora → distribuição do tráfego ao longo do dia.\n",
    "\n",
    "    IPs únicos por hora → quantos visitantes distintos acessaram o site a cada hora.\n",
    "    \n",
    "    Salva todos os resultados em Parquet, permitindo consultas futuras eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bbab1",
   "metadata": {},
   "source": [
    "Conclusão:\n",
    "\n",
    "O Spark permite analisar grandes volumes de logs de forma rápida, sem a necessidade de loops manuais. Com ele, é possível identificar as páginas mais populares, detectar problemas no servidor, como erros 500 ou 404, observar os picos de acesso ao longo do dia e contabilizar o número de visitantes únicos. O código é totalmente escalável, funcionando da mesma forma para logs de megabytes ou gigabytes, e demonstra como utilizar DataFrame e SQL para extrair informações estruturadas a partir de dados semi-estruturados, como os arquivos de log."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
