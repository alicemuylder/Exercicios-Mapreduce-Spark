{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc79063",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c3c602",
   "metadata": {},
   "source": [
    "2. Como o algoritmo se comportaria com 1 milhão de documentos?\n",
    "\n",
    "Com 1 milhão de documentos, o algoritmo ainda funcionaria, mas ficaria lento e exigiria bastante memória, porque tudo é processado de forma sequencial e armazenado em listas e dicionários locais. Cada documento é lido e suas palavras são mantidas na memória até o final, o que não é eficiente em larga escala. Para esse caso o ideal sera usar o spark por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40637844",
   "metadata": {},
   "source": [
    "3. Onde estão os gargalos de performance?\n",
    "\n",
    "O primeiro gargalo está na parte, grouped = shuffle_and_sort(mapped), porque nela todas as palavras são agrupadas. Essa etapa consome bastante memória, pois precisa manter todas as palavras e seus valores em um dicionário. E o segundo gargalo é na a ordenação final, que fica mais pesada conforme aumenta o número de palavras únicas. E além disso, o fato de o código rodar de forma sequencial também limita o desempenho."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
